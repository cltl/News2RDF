{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credits go to Emiel van Miltenburg for the script to convert the spacy output to NAF:\n",
    "\n",
    "https://github.com/evanmiltenburg/SpaCy-to-NAF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from spacy.en import English\n",
    "import json\n",
    "from lxml import etree\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy_to_naf\n",
    "import semeval_classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_first_x_files(size):\n",
    "    path = 'signalmedia-1m.jsonl'\n",
    "    with open(path) as infile:\n",
    "        for counter, line in enumerate(infile, 1):\n",
    "            article = json.loads(line)\n",
    "            \n",
    "            if counter == size: \n",
    "                break\n",
    "            else:\n",
    "                yield article\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "the_news_items = {}\n",
    "the_collection = 'SignalMedia'\n",
    "\n",
    "for article in process_first_x_files(100):\n",
    "    \n",
    "    identifier = article['id']\n",
    "    \n",
    "    a_news_item = semeval_classes.NewsItem(\n",
    "        identifier = identifier,\n",
    "        collection = the_collection,\n",
    "        dct = article['published'],\n",
    "        publisher = article['source'])\n",
    "    \n",
    "    # add entity mention\n",
    "    naf = spacy_to_naf.text_to_NAF(article['content'], nlp)\n",
    "\n",
    "    iden2wf_el = {int(wf_el.get('id')[1:]) : wf_el\n",
    "                  for wf_el in naf.iterfind('text/wf')}\n",
    "    for entity_el in naf.iterfind('entities/entity'):\n",
    "        entity_type = entity_el.get('type')\n",
    "        idens = [int(t_id.get('id')[1:]) \n",
    "                 for t_id in entity_el.iterfind('references/span/target')]\n",
    "        \n",
    "        # get mention\n",
    "        mention = ' '.join([iden2wf_el[iden].text\n",
    "                            for iden in idens])\n",
    "        \n",
    "        # get sentence id\n",
    "        sent_ids = [iden2wf_el[iden].get('sent')\n",
    "                    for iden in idens]\n",
    "        assert len(set(sent_ids)) == 1, 'in %s, entity in multiple sentences' % article['content']\n",
    "        sent_id = sent_ids[0]\n",
    "        \n",
    "        # get start and end offset\n",
    "        wf_el = iden2wf_el[idens[0]]\n",
    "        begin_index = int(wf_el.get('offset'))\n",
    "        \n",
    "        if len(idens) == 1:\n",
    "            end_index = begin_index + int(wf_el.get('length'))\n",
    "        else:\n",
    "            end_wf_el = iden2wf_el[idens[-1]]\n",
    "            end_index = int(end_wf_el.get('offset')) + int(end_wf_el.get('length'))\n",
    "\n",
    "        entity_mention_obj = semeval_classes.EntityMention(\n",
    "            sentence=sent_id,\n",
    "            mention=mention,\n",
    "            the_type=entity_type,\n",
    "            begin_index=begin_index,\n",
    "            end_index=end_index)\n",
    "        a_news_item.entity_mentions.add(entity_mention_obj)\n",
    "        \n",
    "    the_news_items[identifier] = a_news_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('news_items.bin', 'wb') as outfile:\n",
    "    pickle.dump(the_news_items, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running entire datasets using miltiple threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%time for text in process_first_x_files(1000): nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#texts = process_first_x_files(1000)\n",
    "#%time for doc in nlp.pipe(texts, n_threads=16, batch_size=1000): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
